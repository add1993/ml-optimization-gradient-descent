# -*- coding: utf-8 -*-
"""MLOptAssignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hwSvFyV9vuhr-tFJwHd9cAcrOUx4gqCg
"""

"""
Q4. Compare convergence for various gradient descent algorithm. 

Comparison :
Gradient descent : Gradient descent with fixed step size does not convege fast due to fixed learning rate. So we need to try different
 learning rates to get better convergence.

Armijo line search : This adds a line search logic to update the learning rate based on our progress. It works better than the gradient
descent algorithm.

Accelerated gradient descent : It uses extrapolation to get better estimate of alpha. Combined with line search it works really well 
and converges really fast.

Conjugate gradient descent : Conjugate gradient descent uses newton approximation to move near to the optimal solution. It works better
than armijo line search and comparable to accelerated gradient descent. Used fletcher reeves variant for the update beta.

Barzilai gradient descent : Barzilai Borwein used alpha update formula to reset alpha in every iteration. It works better
than armijo line search and comparable to accelerated gradient descent and conjugate gradient descent. Used fletcher reeves variant for the update beta.
"""
from scipy.io import loadmat
import math 
import pandas as pd
import numpy as np

def logisticLoss(w, X, y, lam):
    m = X.shape[0]
    Xw = np.matmul(X, w)

    loss = np.sum(np.log(1 + np.exp(-1 * y * (Xw))))
    loss += lam * (np.linalg.norm(w) ** 2)

    grad = np.zeros(w.shape)
    cc = np.array(-1 * y * (Xw), dtype=np.float128)
    nexp = np.exp(cc)
    grad = (-1) * np.matmul(X.T, y * nexp / (1 + nexp))
    grad += 2 * lam * w
    return loss, grad

def hingeLoss(w, X, y, lam):
  m = X.shape[0]
  Xw = X.dot(w)
  tmp = (1 - 1* y * Xw)
  idx = np.where(tmp > 0)
  loss = np.sum(tmp[idx])
  loss += lam*(np.linalg.norm(w)**2)

  grad = np.zeros(w.shape)
  rows = X.shape[0]
  cols = X.shape[1]
  grad = np.sum(np.where(np.repeat((1 - y*np.matmul(X, w)), cols).reshape(rows, cols) > 0, 
                         (-1)*np.multiply(np.repeat(y, cols).reshape(rows, cols), 
                                          X), 0), axis = 0)

  grad = 2*lam*w + grad
  return loss, grad

def gd(funObj, w, maxEvals, alpha, X, y, lam, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []

    while (1):
        w = w - alpha*g
        [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
        optCond = np.linalg.norm(g, np.inf)

        if verbosity > 0:
            print(funEvals, alpha, f, optCond)

        
        funEvals = funEvals + 1
        funVals.append(f)
        if (optCond < 0.001) or (funEvals > maxEvals):
            break

    return funVals

def gd_with_line_searchv4(funObj, w, maxEvals, alpha, X, y, lam, gamma, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []
    f_old = f
    if alpha == 0:
      alpha = 1.0/np.linalg.norm(g, np.inf)
    
    while (1):
        wp = w - alpha * g
        [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
        funEvals = funEvals + 1
        funVals.append(fp)
        optCond = np.linalg.norm(gp, np.inf)
        if verbosity > 0:
            print(funEvals, alpha, fp, optCond)
        if funEvals > maxEvals:
            break

        gTg = np.dot(g.T, g)
            
        while fp > f - gamma * alpha * gTg:
            alpha = (alpha * alpha * gTg)/(2*(fp + alpha * gTg - f))
            if alpha > 3 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
            wp = w - alpha * g
            [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
            #funEvals = funEvals + 1
            #optCond = np.linalg.norm(gp, np.inf)
            #funVals.append(fp)
            #if verbosity > 0:
            #  print(funEvals, alpha, fp, optCond)
            #if funEvals > maxEvals:
            #  break

        f_old = f
        f = fp
        g = gp
        w = wp
        alpha = min(1, (2.0 * (f_old - f)) / gTg)
        if alpha > 3 or alpha < 1e-6:
          alpha = 1.0/np.linalg.norm(g, np.inf)
        
        optCond = np.linalg.norm(g, np.inf)

        if (optCond < 0.001) or (funEvals > maxEvals):
            break

    return funVals

def accelerated_gradient_descent(funObj, w, maxEvals, alpha, X, y, lam, gamma, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []
    f_old = f
    t = 1
    if alpha == 0:
      alpha = 1.0/np.linalg.norm(g, np.inf)
    w_old = 0
    yy = w
    while (1):
        if funEvals > 1:
            tp = (1.0 + math.sqrt(1 + 4 * t * t))/2.0
            yy = w + ((t-1)/tp)*(w - w_old)
            t = tp
            [f, g] = funObj(np.array(yy), np.array(X), np.array(y), lam)
            funEvals = funEvals + 1
            funVals.append(f)
            optCond = np.linalg.norm(g, np.inf)
            if verbosity > 0:
              print(funEvals, alpha, f, optCond)
            if funEvals > maxEvals:
              break

        w_old = w
        wp = yy - alpha * g
        [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
        funEvals = funEvals + 1
        funVals.append(fp)
        optCond = np.linalg.norm(gp, np.inf)
        if verbosity > 0:
            print(funEvals, alpha, fp, optCond)
        if funEvals > maxEvals:
            break
        gTg = np.dot(g.T, g)
        while fp > f - gamma * alpha * gTg:
            alpha = (alpha * alpha * gTg)/(2*(fp + alpha * gTg - f))
            if alpha > 2 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
            wp = yy - alpha * g
            [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
            #funEvals = funEvals + 1
            #funVals.append(fp)
            #optCond = np.linalg.norm(gp, np.inf)
            #if verbosity > 0:
            #  print(funEvals, alpha, fp, optCond)
            #if funEvals > maxEvals:
            #  break

        f_old = f
        f = fp
        #g = gp
        w = wp
        alpha = min(1, (2.0 * (f_old - f)) / gTg)
        if alpha > 2 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
        
        optCond = np.linalg.norm(g, np.inf)

        if (optCond < 0.001) or (funEvals > maxEvals):
            break

    return funVals

# Conjugate Gradient Descent
def conjugate_gd(funObj, w, maxEval, alpha, X, y, lam, gamma, verbosity):
  [fo,go] = funObj(np.array(w),np.array(X),np.array(y),lam)
  funEvals = 1
  funVals = []
  dcurr = -go
  gprev = np.linalg.norm(go)**2
  alpha = 1/np.linalg.norm(go)
  while(1):
    [f,g] = funObj(np.array(w),np.array(X),np.array(y),lam)
    optCond = np.linalg.norm(g, np.inf)
    if verbosity > 0:
      print(funEvals, alpha, f, optCond)
    wp = w + alpha*dcurr
    [fp,gp] = funObj(np.array(wp),np.array(X),np.array(y),lam)
    while (fp > f - gamma*alpha*(np.linalg.norm(g))):
      num = alpha**2 *((np.dot(g.T,g)).item())
      den = fp + alpha*(np.dot(g.T,g).item()) - f
      alpha = num /(2*den)
      #if alpha > 3 or alpha <= 1e-6:
      #  alpha = 1/np.linalg.norm(go)
      wp = w + alpha*dcurr
      [fp,gp] = funObj(np.array(wp),np.array(X),np.array(y),lam)
    
    alpha = min(1,2*(f - fp)/np.dot(g.T,g))
    if alpha > 2 or alpha <= 1e-6:
      alpha = 1/np.linalg.norm(go)
    gcurr = np.linalg.norm(g)**2
    beta = float(gcurr*1.0)/float(gprev)
    dn = -g + beta*dcurr
    gprev = gcurr
    dcurr = dn
    f = fp
    w = wp
    funEvals = funEvals + 1
    funVals.append(f)
    if((optCond < 1e-3) or (funEvals > maxEval)):
      break
    
  return funVals

def conjugate_gradient_descent_deprecated(funObj, w, maxEvals, alpha, X, y, lam, gamma, verbosity):
    [f, g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []
    f_old = f
    d = -g
    deltanew = np.dot(g.T, g)
    delta0 = deltanew
    if alpha == 0:
      alpha = 1.0 / np.linalg.norm(g, np.inf)

    while (1):
        wp = w + alpha * d
        [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
        funVals.append(fp)
        funEvals = funEvals + 1
        optCond = np.linalg.norm(gp, np.inf)
        if verbosity > 0:
            print(funEvals, alpha, fp, optCond)

        if funEvals > maxEvals:
            break
        gTg = np.dot(g.T, g)
        while fp > f - gamma * alpha * gTg:
            alpha = (alpha * alpha * gTg) / (2 * (fp + alpha * gTg - f))
            if alpha > 2 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
            wp = w + alpha * g
            [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
            #funVals.append(fp)
            #funEvals = funEvals + 1
            #optCond = np.linalg.norm(gp, np.inf)
            #if verbosity > 0:
            #  print(funEvals, alpha, fp, optCond)
            #if funEvals > maxEvals:
            #  break

        f_old = f
        f = fp
        g = gp
        w = wp
        alpha = min(1, (2.0 * (f_old - f)) / gTg)
        if alpha > 2 or alpha < 1e-6:
          alpha = 1.0/np.linalg.norm(g, np.inf)
        
        deltaold = deltanew
        deltanew = np.dot(g.T, g)
        beta = float(deltanew*1.0 / float(deltaold))
        d = -g + beta*d
        optCond = np.linalg.norm(g, np.inf)

        if (optCond < 0.001) or (funEvals > maxEvals):
            break

    return funVals

def barzilai_borwein(funObj, w, maxEvals, alpha, X, y, lam, gamma, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []
    f_old = f
    #t = 1
    if alpha == 0:
        alpha = 1.0/np.linalg.norm(g, np.inf)
    #g_old = g
    w_old = w
    while (1):
        if funEvals > 1:
            g_diff = g - g_old
            w_diff = w - w_old
            alpha = np.asscalar((np.dot(w_diff.T, w_diff)*1.0)/(np.dot(w_diff.T, g_diff)))
            if alpha > 2 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
        d = g
        wp = w - alpha * d
        [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
        funVals.append(fp)
        optCond = np.linalg.norm(gp, np.inf)
        if verbosity > 0:
            print(funEvals, alpha, fp, optCond)
        funEvals = funEvals + 1
        if funEvals > maxEvals:
            break
        gTg = np.dot(g.T, d)
        while fp > f - gamma * alpha * gTg:
            alpha = (alpha * alpha * gTg)/(2*(fp + alpha * gTg - f))
            if alpha > 2 or alpha < 1e-6:
              alpha = 1.0/np.linalg.norm(g, np.inf)
            wp = w - alpha * d
            [fp, gp] = funObj(np.array(wp), np.array(X), np.array(y), lam)
            #funEvals = funEvals + 1
            #funVals.append(fp)
            #optCond = np.linalg.norm(gp, np.inf)
            #if verbosity > 0:
            #  print(funEvals, alpha, fp, optCond)
            #if funEvals > maxEvals:
            #  break
        g_old = g
        f_old = f
        w_old = w
        f = fp
        g = gp
        w = wp
        
        #alpha = min(1, (2.0 * (f_old - f)) / gTg)
        #f_old = f
        optCond = np.linalg.norm(g, np.inf)

        if (optCond < 0.001) or (funEvals > maxEvals):
            break

    return funVals

"""
Running gradient descent algorithms for logistic loss
"""
X = loadmat(r"train.mat")
y = np.loadtxt(r"train.targets")
X = X['X'].todense()
w = np.random.random(X.shape[1])
y = np.array(y)
X_main = X
Y_main = y

num_iterations = 250
gamma = 0.0001
verbosity = 1
gd_alpha = 0.00001
gd_with_ls_alpha = 0
lam = 0.01


print("Running gradient descent algorithms for logistic loss")
print("Running normal gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funV1 = gd(logisticLoss, w, num_iterations, gd_alpha, X, y, lam, verbosity)

print("Running gradient descent with armijo backtracking line search")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funV2 = gd_with_line_searchv4(logisticLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running accelerated gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funV3 = accelerated_gradient_descent(logisticLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running conjugate gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funV4 = conjugate_gd(logisticLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running barzilai borwein gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funV5 = barzilai_borwein(logisticLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

X_ = []
for i in range(num_iterations):
  X_.append(i+1)

from matplotlib import pyplot as plt

F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True)

plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Logistic Loss function value over iterations vs Iterations for different gradient desc')
plt.plot(X_, funV1, '-r', label='Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funV2, '-g', label='Gradient descent with armijo line search', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funV3, '-b', label='Accelerated gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funV4, '-c', label='Conjugate gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funV5, '-k', label='Barzilai borwein gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.legend(loc="upper right")
plt.show()

"""
Running gradient descent techniques for hinge loss
"""

X = loadmat(r"train.mat")
y = np.loadtxt(r"train.targets")
X = X['X'].todense()
w = np.random.random(X.shape[1])
y = np.array(y)
X_main = X
Y_main = y

num_iterations = 250
gamma = 0.0001
verbosity = 1
gd_alpha = 0.00001
gd_with_ls_alpha = 0
lam = 0.01

print("Running gradient descent algorithms for hinge loss")
print("Running normal gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funH1 = gd(hingeLoss, w, num_iterations, gd_alpha, X, y, lam, verbosity)

print("Running gradient descent with armijo line search v4")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funH2 = gd_with_line_searchv4(hingeLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running accelerated gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funH3 = accelerated_gradient_descent(hingeLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running conjugate gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funH4 = conjugate_gd(hingeLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

print("Running barzilai borwein gradient descent")
w = np.random.random(X.shape[1])
X = X_main
y = Y_main
funH5 = barzilai_borwein(hingeLoss, w, num_iterations, gd_with_ls_alpha, X, y, lam, gamma, verbosity)

X_ = []
for i in range(num_iterations):
  X_.append(i+1)

from matplotlib import pyplot as plt

F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True)


plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for different gradient desc')
plt.plot(X_, funH1, '-r', label='Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funH2, '-g', label='Gradient descent with armijo line search', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funH3, '-b', label='Accelerated gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funH4, '-c', label='Conjugate gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(X_, funH5, '-k', label='Barzilai borwein gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.legend(loc="upper right")
plt.show()

"""
Q5. Compare gradient descent and proximal gradient descent.

Proximal gradient descent works better than normal gradient descent by calculating the prox operator. 
It is evident from the plot that proximal gradient descent works better.
"""
from scipy.io import loadmat
import math
import pandas as pd
import numpy as np

def logisticLossL1(w, X, y, lam):
    m = X.shape[0]
    Xw = np.matmul(X, w)

    loss = np.sum(np.log(1 + np.exp(-1 * y * (Xw))))
    loss += lam * (np.linalg.norm(w))

    grad = np.zeros(w.shape)
    nexp = np.exp(-1 * y * (Xw))
    grad = (-1) * np.matmul(X.T, y * nexp / (1 + nexp))
    grad += w
    return loss, grad

def logisticLoss(w, X, y, lam):
    m = X.shape[0]
    Xw = np.matmul(X, w)

    loss = np.sum(np.log(1 + np.exp(-1 * y * (Xw))))
    #loss += lam * (np.linalg.norm(w))

    grad = np.zeros(w.shape)
    nexp = np.exp(-1 * y * (Xw))
    grad = (-1) * np.matmul(X.T, y * nexp / (1 + nexp))
    #grad += w
    return loss, grad

def gd(funObj, w, maxEvals, alpha, X, y, lam, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []

    while (1):
        w = w - alpha*g
        [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
        optCond = np.linalg.norm(g, np.inf)

        if verbosity > 0:
            print(funEvals, alpha, f, optCond)

        
        funEvals = funEvals + 1
        funVals.append(f)
        if (optCond < 0.001) or (funEvals > maxEvals):
            break
        

    return funVals

def proxNorm1(y, lamda):
    return np.sign(y)*np.maximum(np.zeros(np.shape(y)), np.abs(y)-lamda)
    #return np.sign(y)*(np.abs(y)-lamda).astype(int)
    

def proximal_gd(funObj, w, maxEvals, alpha, X, y, lam, verbosity):
    [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
    funEvals = 1
    funVals = []
    lam_np = np.ones(w.shape)
    lam_np = lam_np * lam

    while (1):
        wk = w - alpha * g
        wk = proxNorm1(np.array(wk), lam*alpha)
        [f,g] = funObj(np.array(w), np.array(X), np.array(y), lam)
        w = wk
        optCond = np.linalg.norm(g, np.inf)

        if verbosity > 0:
            print(funEvals, alpha, f, optCond)

        
        funEvals = funEvals + 1
        funVals.append(f)
        if (optCond < 0.001) or (funEvals > maxEvals):
            break
        

    return funVals

X = loadmat(r"train.mat")
y = np.loadtxt(r"train.targets")

X = X['X'].todense()
w = np.random.random(X.shape[1])
y = np.array(y)

num_iterations = 250
gamma = 0.0001
verbosity = 1
gd_alpha = 0.00001
gd_with_ls_alpha = 0
lam = 1000
results_p = []

print("Running normal gradient descent")
w = np.random.random(X.shape[1])
funV = gd(logisticLossL1, w, num_iterations, gd_alpha, X, y, lam, verbosity)
results_p.append(funV)

print("Running proximal gradient descent")
w = np.random.random(X.shape[1])
funV = proximal_gd(logisticLoss, w, num_iterations, gd_alpha, X, y, lam, verbosity)
results_p.append(funV)

X_ = []
for i in range(num_iterations):
  X_.append(i+1)

from matplotlib import pyplot as plt
import numpy as np

F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True)
Xn = np.arange(num_iterations)
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Logistic Loss function value over iterations vs Iterations for gradient desc')
plt.plot(Xn, results_p[0], '-r', label='Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.plot(Xn, results_p[1], '-c', label='Proximal Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.legend(loc="upper right")
plt.show()

"""from matplotlib import pyplot as plt

F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True)


plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Logistic Loss function value over iterations vs Iterations for barzilai borwein gradient desc')
plt.plot(X_, funV5, '-b', label='Barzilai borwein gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.legend(loc="upper right")
plt.show()

from matplotlib import pyplot as plt



F = plt.gcf()
Size = F.get_size_inches()
F.set_size_inches(Size[0]*2, Size[1]*2, forward=True)

plt.legend(loc="upper left")
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for gradient desc')
plt.plot(X_, results2[0], '-r', label='Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.show()
plt.clf()

plt.legend(loc="upper left")
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for Gradient descent with armijo')
plt.plot(X_, results2[1], '-b', label='Gradient descent with armijo line search', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.show()
plt.clf()

plt.legend(loc="upper left")
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for Accelerated Gradient descent')
plt.plot(X_, results3[2], '-c', label='Accelerated Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.show()
plt.clf()

plt.legend(loc="upper left")
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for Conjugate Gradient descent')
plt.plot(X_, results3[3], '-m', label='Conjugate Gradient descent', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.show()
plt.clf()

plt.legend(loc="upper left")
plt.xlabel('Iterations')
plt.ylabel('Loss function value')
plt.title('Hinge Loss function value over iterations vs Iterations for Barzilai Borwein')
plt.plot(X_, results2[4], '-k', label='Barzilai Borwein', marker='o', markerfacecolor ='g', markeredgecolor ='g', markersize=1)
plt.show()
"""